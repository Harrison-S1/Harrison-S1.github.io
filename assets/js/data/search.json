[ { "title": "Atlas project - part two", "url": "/posts/atlas-project-part-two/", "categories": "aws, howto's, linux, cloud, terraform, docker", "tags": "aws, linux, howto's, sysops, cloud, terraform, docker", "date": "2023-04-24 14:00:00 +0000", "snippet": "OK, so we have our docker image, and we are storing it in the Docker repository. Now what.Well we want to create some cloud infrastructure, so we can deploy our game, but we will be using Terraform to code up our infrastructure. Note, for this, I’m going to use the cloud playground from my subscription to A Cloud Guru. You can use this project on with the AWS free tier account, so if you need to go and sign up.TerraformLet’s create a new folder, name it whatever and create your main.tf file with the following. In here we are just defining around AWS instance.resource \"aws_instance\" \"dev_node\" { instance_type = \"t2.micro\" ami = data.aws_ami.server_ami.id vpc_security_group_ids = [aws_security_group.main_aws_sg.id] iam_instance_profile = aws_iam_instance_profile.dev_profile.name subnet_id = aws_subnet.main_public_subnet.id user_data = file(\"userdata.tpl\") root_block_device { volume_size = 10 } tags = { Name = \"dev-node\" }}From here on out, just create the file name and add the code.vpc.tf Here we are defining everything we need for our network in AWS.resource \"aws_vpc\" \"mainvpc\" { cidr_block = \"10.123.0.0/16\" enable_dns_hostnames = true enable_dns_support = true tags = { Name = \"dev\" }}resource \"aws_subnet\" \"main_public_subnet\" { vpc_id = aws_vpc.mainvpc.id cidr_block = \"10.123.1.0/24\" map_public_ip_on_launch = true availability_zone = \"us-east-1a\" tags = { Name = \"dev-public\" }}resource \"aws_internet_gateway\" \"main_internet_gateway\" { vpc_id = aws_vpc.mainvpc.id tags = { Name = \"dev-igw\" }}resource \"aws_route_table\" \"main_public_rt\" { vpc_id = aws_vpc.mainvpc.id tags = { Name = \"dev_public_rt\" }}resource \"aws_route\" \"default_route\" { route_table_id = aws_route_table.main_public_rt.id destination_cidr_block = \"0.0.0.0/0\" gateway_id = aws_internet_gateway.main_internet_gateway.id}resource \"aws_route_table_association\" \"main_public_assoc\" { subnet_id = aws_subnet.main_public_subnet.id route_table_id = aws_route_table.main_public_rt.id}variable.tf Here, we define any variables being used. In this case, the host OSvariable \"host_os\" { type = string default = \"linux\" description = \"default os being used to deploy\"}sg.tf Here we are creating the security group that will be attacked to our AWS instance. For the sack of the project, it has been left open to all traffic. But we will look at securing it later on.resource \"aws_security_group\" \"main_aws_sg\" { name = \"dev_sg\" description = \"dev security group\" vpc_id = aws_vpc.mainvpc.id ingress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] }}iam.tf Here we are creating the IAM role of the instance. Note that this isnt required for the current interation, but if you where adding a S3 bucket, this will give the ec2 instance access and well as access to CloudWatchresource \"aws_iam_role\" \"EC2_dev_role\" { name = \"EC2_dev_role\" assume_role_policy = &lt;&lt;EOF{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"sts:AssumeRole\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Effect\": \"Allow\", \"Sid\": \"\" } ]}EOF tags = { tag-key = \"Cloudwatch\" }}resource \"aws_iam_instance_profile\" \"dev_profile\" { name = \"dev_profile\" role = aws_iam_role.EC2_dev_role.name}resource \"aws_iam_role_policy\" \"s3-dev\" { name = \"s3-dev\" role = aws_iam_role.EC2_dev_role.id policy = &lt;&lt;EOF{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\", \"s3-object-lambda:*\" ], \"Resource\": \"*\" } ]}EOF}datasource.tf Here we are declaring the data source of our AMI image.data \"aws_ami\" \"server_ami\" { most_recent = true owners = [\"099720109477\"] filter { name = \"name\" values = [\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"] }}outputs.tf Sometimes we need some information about our instance after it has been created, such as its IP or instance ID.output \"dev_ip\" { description = \"Output the public IP of the dev node instance\" value = aws_instance.dev_node.public_ip}output \"instance_id\" { description = \"ID of the EC2 instance\" value = aws_instance.dev_node.id}userdata.tpl If you have created your own docker reg, you can the script to pull that image inseed.#!/bin/bashsudo apt-get update -y &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-commonwget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.debsudo dpkg -i amazon-cloudwatch-agent.debsudo curl -o /opt/aws/amazon-cloudwatch-agent/amazon-cloudwatch-agent.json https://raw.githubusercontent.com/Harrison-S1/terraform/master/cloudwatch-config.jsonsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/amazon-cloudwatch-agent.json curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpgecho \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get update -y &amp;&amp; sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-pluginsudo apt install unzip -ycurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/home/ubuntu/awscliv2.zip\"sudo usermod -aG docker ubuntu # &amp;&amp; sudo reboot nowsudo docker pull harrisons1/2048sudo docker run --name 2048 -p 80:80 -d harrisons1/2048providers.tf OK, so we will need to set up Terraform Cloud to store the state file for this project. Make sure you put your organization and workspaces you have set up in Terraform Cloud into this file.terraform { required_providers { aws = { source = \"hashicorp/aws\" } random = { source = \"hashicorp/random\" version = \"3.0.1\" } } required_version = \"~&gt; 1.0\" backend \"remote\" { organization = \"CHANGEME\" workspaces { name = \"CHANGEME\" } }}provider \"aws\" { region = \"us-east-1\"}Setting up your AWS accountYou will need to create a user in AWS to deploy this. Log into your AWS account. Navigate to IAM and select Users on the left-hand menu Select “Add users” Create the username Select “Access key - Programmatic access” and click next Select “Attach existing policies directly” and use the “AdministratorAccess” policy Don’t do this for productions Tags are optional, so just click next, then create userNow on the next page you need to add the details from user, access key ID and Secret access key to the AWS credentials file.~/.aws/credentialsFor example[terraformuser]# This key identifies your AWS account.aws_access_key_id = QKIAUDRQURVLS2JCIV4# Treat this secret key like a password. Never share it or store it in source# control. If your secret key is ever disclosed, immediately use IAM to delete# the key pair and create a new one.aws_secret_access_key = nBhpjpTCyerter3sdf0l3QZ7zxV78ptCFSetting up your Terraform Cloud Here, I’m assuming you already have an AWS account set up. This is part of the backend that is defined in providers.tf. You will need to go back to the providers.tf file and change the organization and workspaces name.In your Terraform Cloud, create a new workspace Select API-driven workflow Name the workspace and select the project (default is fine) Click on Create workspace In the left-hand menu select Variables Click Add variable Add the AWS_ACCESS_KEY_ID key, mark it as a Environment variable and also mark it as Sensitive Do the same for the AWS_SECRET_ACCESS_KEY variable Now lets create our TF_API_TOKEN for GitHub Actions. Click on your profile picture at the top left and select User Settings On the left-hand menu select Tokens and click on Create an API token Name it whatever you want, and copy the token to a notepadNow, in your terminal, run terraform init. Then terraform fmt to check the format. Once it is happy, you can run a terraform plan. Check you are happy with the output of the plan, then you can apply it with terraform apply Congratulations you now have a running game of 2048 in docker on an ec2 instance, with the support of AWS infrastructure, build with Terraform.In part 3 we’ll look at setting up some CI with GitHub actions." }, { "title": "Atlas Project - part one", "url": "/posts/atlas-project-part-one/", "categories": "aws, howto's, linux, cloud, terraform, docker", "tags": "aws, linux, howto's, sysops, cloud, terraform, docker", "date": "2023-04-17 14:00:00 +0000", "snippet": "Often in tech it is hard to see the wood through the trees and stringing what you know and learned together can be difficult. So here is my mini project to learn more about GitHub actions Terraform Cloud and to brush up on some Docker and Terraform skills.I’ll be walking you though how to deploy a browser based puzzle game called 2048 which by now is a fork, of a fork of a fork, On AWS using Docker, Terraform, Terraform Cloud and GitHub Actions.Getting starting, I assume you know enough Linux to be dangerous, how to install Docker, Terraform on your machine, and you have or will create GitHub account and Terraform Cloud account. AWS you can either use your own account or you have access to one through a training provider like A Cloud Guru.DockerI like to have control over the images, but you can either use mine or create your own and push it up to the docker registry.Building the imageThis one is nice and straight forward:Clone the repocd 2048Check the Dockerfile, which will contain the following:FROM nginx:latestCOPY 2048 /usr/share/nginx/htmlEXPOSE 80Now run the build with:docker build .Your output will look something like this[+] Building 7.0s (8/8) FINISHED =&gt; [internal] load .dockerignore 0.1s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load build definition from Dockerfile 0.1s =&gt; =&gt; transferring dockerfile: 97B 0.0s =&gt; [internal] load metadata for docker.io/library/nginx:latest 1.7s =&gt; [auth] library/nginx:pull token for registry-1.docker.io 0.0s =&gt; [1/2] FROM docker.io/library/nginx:latest@sha256:2ab30d6ac53580a6db8b657abf0f68d75360ff5cc1670a85acb5bd85ba1b19c0 4.3s =&gt; =&gt; resolve docker.io/library/nginx:latest@sha256:2ab30d6ac53580a6db8b657abf0f68d75360ff5cc1670a85acb5bd85ba1b19c0 0.0s =&gt; =&gt; sha256:bfb112db4075460ec042ce13e0b9c3ebd982f93ae0be155496d050bb70006750 1.57kB / 1.57kB 0.0s =&gt; =&gt; sha256:080ed0ed8312deca92e9a769b518cdfa20f5278359bd156f3469dd8fa532db6b 7.92kB / 7.92kB 0.0s =&gt; =&gt; sha256:2ab30d6ac53580a6db8b657abf0f68d75360ff5cc1670a85acb5bd85ba1b19c0 1.86kB / 1.86kB 0.0s =&gt; =&gt; sha256:f1f26f5702560b7e591bef5c4d840f76a232bf13fd5aefc4e22077a1ae4440c7 31.41MB / 31.41MB 1.2s =&gt; =&gt; sha256:7f7f30930c6b1fa9e421ba5d234c3030a838740a22a42899d3df5f87e00ea94f 25.58MB / 25.58MB 1.0s =&gt; =&gt; sha256:2836b727df80c28853d6c505a2c3a5959316e48b1cff42d98e70cb905b166c82 626B / 626B 0.3s =&gt; =&gt; sha256:e1eeb0f1c06b25695a5b9df587edf4bf12a5af9432696811dd8d5fcfd01d7949 956B / 956B 0.5s =&gt; =&gt; sha256:86b2457cc2b0d68200061e3420623c010de5e6fb184e18328a46ef22dbba490a 772B / 772B 0.7s =&gt; =&gt; sha256:9862f2ee2e8cd9dab487d7dc2152a3f76cb503772dfb8e830973264340d6233e 1.40kB / 1.40kB 0.9s =&gt; =&gt; extracting sha256:f1f26f5702560b7e591bef5c4d840f76a232bf13fd5aefc4e22077a1ae4440c7 0.9s =&gt; =&gt; extracting sha256:7f7f30930c6b1fa9e421ba5d234c3030a838740a22a42899d3df5f87e00ea94f 0.5s =&gt; =&gt; extracting sha256:2836b727df80c28853d6c505a2c3a5959316e48b1cff42d98e70cb905b166c82 0.0s =&gt; =&gt; extracting sha256:e1eeb0f1c06b25695a5b9df587edf4bf12a5af9432696811dd8d5fcfd01d7949 0.0s =&gt; =&gt; extracting sha256:86b2457cc2b0d68200061e3420623c010de5e6fb184e18328a46ef22dbba490a 0.0s =&gt; =&gt; extracting sha256:9862f2ee2e8cd9dab487d7dc2152a3f76cb503772dfb8e830973264340d6233e 0.0s =&gt; [internal] load build context 0.1s =&gt; =&gt; transferring context: 600.50kB 0.0s =&gt; [2/2] COPY 2048 /usr/share/nginx/html 0.2s =&gt; exporting to image 0.6s =&gt; =&gt; exporting layers 0.6s =&gt; =&gt; writing image sha256:88c5f79cbf57badc37bfa77578431557d34dd6fef640574ae4646b6f1a2a0eae Run docker images and you will see your newly build image.REPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 88c5f79cbf57 About a minute ago 143MBYou will want to tag your image to make it easier to know what one you are using, etc. Run the following command to do so.docker build -t yourusername/repository-name .Run the docker imagescommand again, and you will see the image with what you have just tagged it with.Now you can also test the image to make sure it’s work as it should be. Run the following command: Note, change the [MYIMAGE] to what you have tagged the image asdocker run -p 8089:80 [MYIMAGE]You will now be able to check it in the browser at http://localhost:8089/ From here you can either push that image up to the Docker repository or use the image I created.To use the image I have created, just run:docker pull harrisons1/2048and rundocker run -p 8089:80 harrisons1/2048 OK, so we have our first part of the puzzle, a web based game inside a container. Part two will be setting up the AWS environment with Terraform." }, { "title": "Creating a Storage Gateway in AWS", "url": "/posts/aws-storage-gateway/", "categories": "aws, howto's, linux, cloud, terraform", "tags": "aws, linux, howto's, sysops, cloud, terraform", "date": "2022-12-17 14:00:00 +0000", "snippet": "So what is a Storge Gateway?A Storage Gateway it just a way to connect AWS storage in a traditional way e.g. NFS, SMB etc.Lets build one with Terraform A fully working envriment with a Storage Gateway can be found hereSo a Storage Gateway is built up of the following A instance with a dedicated 150GB cahce drive A role A security group S3 bucket (in this example) S3 policyThe Instance// NOTE: the varable for the instance ami used here will be stored in a datasource.tf file. resource \"aws_instance\" \"dev_gateway\" { instance_type = \"m5.xlarge\" ami = data.aws_ami.server_ami_gw.id key_name = aws_key_pair.clouddev_auth.id vpc_security_group_ids = [aws_security_group.main_gateway_sg.id] iam_instance_profile = aws_iam_instance_profile.dev_profile.name subnet_id = aws_subnet.main_public_subnet.id root_block_device { volume_size = 80 volume_type = \"gp3\" } tags = { Name = \"dev-gateway\" } }// NOTE: This is where you will define the disk used for the cache.resource \"aws_ebs_volume\" \"dev-cache\" { availability_zone = \"eu-west-2a\" size = 150 type = \"gp3\" encrypted = true tags = { Name = \"dev-cache\" } depends_on = [ aws_instance.dev_gateway ]}// NOTE: This is where you will attck the disk to the new instance. resource \"aws_volume_attachment\" \"ebs_att\" { device_name = \"/dev/xvds\" volume_id = aws_ebs_volume.dev-cache.id instance_id = aws_instance.dev_gateway.id}data \"aws_storagegateway_local_disk\" \"example\" { disk_node = \"/dev/xvds\" gateway_arn = aws_storagegateway_gateway.g1.arn}resource \"aws_storagegateway_cache\" \"dev-cache\" { disk_id = data.aws_storagegateway_local_disk.example.id gateway_arn = aws_storagegateway_gateway.g1.arn}The varable for the instance ami will look like this: The ami that is being used has been built by AWS.data \"aws_ami\" \"server_ami_gw\" { most_recent = true owners = [\"029176880048\"] filter { name = \"name\" values = [\"aws-storage-gateway-*\"] }} If you are building this in one .tf file just add the above data source for it run.Security Group// NOTE: Ingress / Egress rules taken from AWS:// https://docs.aws.amazon.com/storagegateway/latest/userguide/Resource_Ports.htmlresource \"aws_security_group\" \"main_gateway_sg\" { name = \"dev-gateway\" description = \"Security Group for NFS File Gateway.\" vpc_id = aws_vpc.mainvpc.id // Activation ingress { protocol = \"tcp\" from_port = 80 to_port = 80 cidr_blocks = [\"0.0.0.0/0\"] } ingress { protocol = \"tcp\" from_port = 443 to_port = 443 cidr_blocks = [\"0.0.0.0/0\"] } // NFS ingress { protocol = \"tcp\" from_port = 20048 to_port = 20048 cidr_blocks = [\"0.0.0.0/0\"] } ingress { protocol = \"udp\" from_port = 20048 to_port = 20048 cidr_blocks = [\"0.0.0.0/0\"] } ingress { protocol = \"tcp\" from_port = 111 to_port = 111 cidr_blocks = [\"0.0.0.0/0\"] } ingress { protocol = \"udp\" from_port = 111 to_port = 111 cidr_blocks = [\"0.0.0.0/0\"] } ingress { protocol = \"tcp\" from_port = 2049 to_port = 2049 cidr_blocks = [\"0.0.0.0/0\"] } ingress { protocol = \"udp\" from_port = 2049 to_port = 2049 cidr_blocks = [\"0.0.0.0/0\"] } // DNS ingress { protocol = \"tcp\" from_port = 53 to_port = 53 cidr_blocks = [\"0.0.0.0/0\"] } ingress { protocol = \"udp\" from_port = 53 to_port = 53 cidr_blocks = [\"0.0.0.0/0\"] } // Allow all egress egress { from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] }}Lets configure the gateway,NFS share &amp; add the S3 policy.resource \"aws_storagegateway_gateway\" \"g1\" { gateway_name = var.gateway_name gateway_timezone = var.gateway_timezone gateway_type = \"FILE_S3\" gateway_ip_address = aws_instance.dev_gateway.public_ip depends_on = [ aws_instance.dev_gateway ]}resource \"aws_storagegateway_nfs_file_share\" \"file_share\" { client_list = [\"0.0.0.0/0\"] gateway_arn = aws_storagegateway_gateway.g1.arn location_arn = aws_s3_bucket.gate_buc.arn role_arn = aws_iam_role.role.arn}resource \"aws_iam_policy\" \"policy\" { name = \"S3_policy\" description = \"Providing limited S3 powers\" # Terraform's \"jsonencode\" function converts a # Terraform expression result to valid JSON syntax. policy = jsonencode({ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"s3:GetAccelerateConfiguration\", \"s3:GetBucketLocation\", \"s3:GetBucketVersioning\", \"s3:ListBucket\", \"s3:ListBucketVersions\", \"s3:ListBucketMultipartUploads\" ], \"Resource\": \"arn:aws:s3:::${var.bucket_name}\", \"Effect\": \"Allow\" }, { \"Action\": [ \"s3:AbortMultipartUpload\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\", \"s3:GetObject\", \"s3:GetObjectAcl\", \"s3:GetObjectVersion\", \"s3:ListMultipartUploadParts\", \"s3:PutObject\", \"s3:PutObjectAcl\" ], \"Resource\": \"arn:aws:s3:::${var.bucket_name}/*\", \"Effect\": \"Allow\" } ] })}Finally lets create our S3 butcketresource \"aws_s3_bucket\" \"gate_buc\" { bucket = var.bucket_name tags = { Name = \"My bucket\" Environment = \"Dev\" }And that’s it, run your terraform apply. You should now have a storage gaetway instance in your EC2 console and your share will be available from the Storage Gateway console with the command you need to mount it. Make sure you read the AWS Docs and the Terraform DocsIs this the best way to access cloud storage?No, I don’t think it is for the following reasons: This solution will be charging you for the instance, ebs volume, S3 bucket and the pulic IP etc. There is a easier way to mount cloud storage on prem and within the cloud environment……Rclone" }, { "title": "CloudWatch Monitor EC2 Linux services and set alarms", "url": "/posts/CloudWatch-Monitor-EC2-Linux-services-and-set-alarms/", "categories": "aws, howto's, linux, cloud, cloudwatch", "tags": "aws, linux, howto's, sysops, cloud, cloudwatch", "date": "2022-10-23 14:00:00 +0000", "snippet": " Prerequisite The Amazon CloudWatch agent is installed You have set up a IAM Role for the CloudWatch agent Set up a service to monitorAt the moment CloudWatch does not monitor services as we would normally expect a monitoring system to do with a agent, is it up or down. So we need to create the metric ourselves.Systemctl can help us there with:systemctl -q is-active httpd.service is-active PATTERN…Check whether any of the specified units are active (i.e.running). Returns an exit code 0 if at least one is active,or non-zero otherwise. Unless –quiet is specified, this willalso print the current unit state to standard output. Using &amp;&amp; echo \"Running:1\" || echo \"Warning:0\"section is using the OR operator, so echo either 1 if systemctl command returns its exit code, the service is up echo “1”‘ OR **   ** …echo “0”, which indicates the service is down. Adding the metric to a log fileNow we want the metric going to a logfile to pull into CloudWatch. In this example I’m using the apache package htpd. Change the log file folder and .log file name to what ever service you are wanting to monitor.mkdir -p /var/log/services/ &amp;&amp; touch /var/log/services/httpd_service.logNow the command will look like this Remember to change the service folder and log name to what service you are usingsystemctl -q is-active httpd.service &amp;&amp; echo \"Running:1\" &gt; /var/log/services/httpd_service.log || echo \"Warning:0\" &gt; /var/log/services/httpd_service.logSo let check, we should either have a 1 (Running) or 0 (Warning)cat /var/log/services/httpd_service.log If nothing is showing, you need to go back and figure out if the path is bad or the directory name is thereAdding this to CRONNow we have data going to the log file, we need to automate it with CRON.As rootcrontab -eand add the following:*/1 * * * * systemctl -q is-active httpd.service &amp;&amp; echo \"Running:1\" &gt;&gt; /var/log/services/httpd_service.log || echo \"Warning:0\" &gt;&gt; /var/log/services/httpd_service.logThis will run every minute for testing purposes, but feel free to adjust it to what you need. Do not forget to change the log file path. The above example is for httpdEdit the CloudWatch agent configNow we need to configure the CloudWatch agent. Edit the CloudWatch agent configuration file.vim /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent.json This is an example of pulling the log metric only{ \"agent\": { \"logfile\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\", \"metrics_collection_interval\": 60, \"run_as_user\": \"root\", . \"debug\": false }, \"logs\": { \"logs_collected\": { \"files\": { . \"collect_list\": [ { \"file_path\": \"/var/log/services/httpd_service.log\", \"log_group_name\": \"/ec2/CloudWatchAgentServiceLog/\", \"log_stream_name\": \"{instance_id}_{hostname}\", \"timezone\": \"Local\" } ] } } }}Now tell the agent to use the new config and restart the agent.sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent.jsonNow wait a minute and you would not have logs being pulled into CloudWatch if there are error it will show in the output of the previous commandYou can check the agents status with:sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a statusSet up logrotateNow we have logs being created and ingested into CloudWatch we need to do some house keeping on the system with logrotate.Create a logrotate config for not just this one service, but all that we may create in the future.touch /etc/logrotate.d/monitored_servicesNow lets edit the configvim /etc/logrotate.d/monitored_services/var/log/services/*log { weekly missingok copytruncate rotate 12 compress delaycompress notifempty} This is a good starting point. it will keep 12 weeks (3 months) worth of logs and compress them. These rules will apply to everything in /var/log/servicess If you need to isolate a service and have a different rule then just separate the log into its own directory. such as /var/log/servicess/httpd/Further readingI would highly recommend going through the docs for Manually create or edit the CloudWatch agent configuration fileSetting up the CloudWatch Metric and AlarmMetricOK so now we want to get alerts from CloudWatch if our service goes down.We need to create a “filter” in CloudWatch and then create a alarm based on the filters status.In the AWS console, navigate toCloudWatch &gt; CloudWatch Logs&gt; Log groups &gt; NAMEOFLOG If you are following along with the example the log name would be ** /ec2/CloudWatchAgentServiceLog/**Then under the Actions menu select Create metric filterNow we need to fill out the requirements for the Define patternFilter pattern enter Warning This is the key word that is is our log file for a service going downClick nextFilter name enter Apache downMetric namespace enter Services We can add other services to the namespace - its just a groupingMetric name enter httpd downMetric value enter 1 Default value – optional enter 0 This will allow us to see when the service goes down on a lined graph, with a 1 indicating the service is down and the indicating normal operationsClick Next.Review and confirmNow when you look at the metric httpd down, you will see when the service has gone down. Set you Period to 1 min and Statistic to max in a line graph view.AlarmTo create our alarm we need to navigate to All alarms on the left hand side panel, then click the orange Create alarm.On the new page select **Select Metric **Now select our Group name that we created earlier, Servicesand select Metric with no dimensions and select httpd down The metric name will change for whatever you have named the metric name to be.Specify metric and conditionsNow we need to set the conditions for the alarm. You can change the metric name here if you wanted to. Statistic select Maximum You want this to be the Maximum, so its either a 1 or a 0, its up or down Period select 1 minutes Change this to meet your needs Threshold type select Static Define the alarm condition select Greater/Equal Define the threshold value enter 1Click NextConfigure actions Alarm state trigger set to In alarm Send a notification to the following SNS topic If you have not already set up a SNS topic you will need to create a new topic. You can also create a OK alert but clicking Add notification and adding the same SNS topic you created. You will then get an alert when the service goes down and comes back up.Click NextAdd name and description Give the alarm a name and description.Click NextPreview and createReview the alarm setting and click Create alarm." }, { "title": "Create a systemd service to monitor Cloudwatch agent", "url": "/posts/Create-a-systemd-service-to-monitor-Cloudwatch-agent/", "categories": "aws, howto's, linux, cloudwatch, cloud", "tags": "aws, linux, howto's, sysops, cloudwatch, cloud", "date": "2022-10-21 21:30:00 +0000", "snippet": "The problem to solveIf you are monitoring an AWS instance that is busy and has other programs accessing the same logs as the Cloudwatch agent then you may run into this error in the agents logs “too many open files” which stops any further information or metrics being sent to Cloudwatch. Not a huge problem, but what happens when everyone has gone home and know one is there to restarts the agent, or worse….what happens if you had a security breach and you need those logs.Part of the SolutionIs to create a small script to monitor the cloudwatch agent log file for that error “too many open files” as done below. The script looks for that error as new logs are entered in to the log file and if that line is seen stop the agent, then start it up and write a log message to say the agent has been restarted.#!/bin/bash tail -fn0 /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log | while read line ; doecho \"${line}\" | grep -i \"too many open files\" &gt; /dev/nullif [ $? = 0 ] ; thensudo amazon-cloudwatch-agent-ctl -a stopsleep 5sudo amazon-cloudwatch-agent-ctl -a startsleep 5sudo echo Agent restarted-$(date +\"%d_%m_%y__%I_%M_%p\") &gt;&gt; /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.logfidoneBut I dont want to have to put that into a cron job and monitor that as well, with other scenarios that can interfere with it. It just becomes messy.The other part of the SolutionLets create a Systemd service to manage it for us as done below.vi /etc/systemd/system/monitor-cloudwatch.service[Unit]Description=Monitors Amazon CloudWatch agent and restart it if error of to many files are open [Service]User=rootWorkingDirectory=/rootExecStart=/root/restartcw.shRestart=always [Install]WantedBy=multi-user.targetOnce you have saved the service file we need to reload the daemon so our new service file gets included.systemctl daemon-reloadNow lets start our new service.system start monitor-cloudwatch.serviceIf you want the service to come up at boot dont forget to enable it.system enable monitor-cloudwatch.serviceCheck the stauts, everything should be happy.system status monitor-cloudwatch.serviceGrab yourself two terminals and you can test it out.tail -f /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.logecho \"too many open files\" &gt;&gt; /opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.logYou should have seen the error message showing up, the agent restarting and a log message to stay your agent has been restarted.Know you will also want to look at getting a notification/email that the agent has been restarted. Im not going to cover it here but you will want to create a metric filter that monitors your log group for specific patterns. AWS has this well documented" }, { "title": "Expanding EBS root volume", "url": "/posts/Expanding-EBS-root-volume/", "categories": "aws, howto's, linux, sysops, cloud", "tags": "aws, linux, howto's, sysops, cloud", "date": "2022-09-17 14:00:00 +0000", "snippet": "Expand the root volume. Then, extend the file system using the Amazon EC2 console (new console) From the Amazon EC2 console, choose Instances from the navigation pane. Select the instance that you want to expand. From the Description tab, choose the volume listed for Block devices. Then, choose the EBS ID. Select the volume. For Actions, choose Modify Volume. In the Size field, enter the Size. If you choose an io1 volume, enter the number of IOPS. Choose Modify, and then choose Yes. Refresh the console page. In the Description tab, the State shows the progress of optimization until the modification is complete. Expand the root volume. Then, extend the file system using the AWS CLIRun a command similar to the following. Replace the with your values:aws ec2 modify-volume --region &lt;regionName&gt; --volume-id &lt;volumeId&gt; --size &lt;newSize&gt; --volume-type &lt;newType&gt; --iops &lt;newIops&gt;Note: To view the progress of your task, run the following command:aws ec2 describe-volumes-modifications --volume &lt;volumeId&gt; --region &lt;region&gt;Extend a Linux file system after resizing a volumeConnect to your instance.To verify the file system and type for each volume, use the df -hT command.df -hTTo check whether the volume has a partition that must be extended, use the lsblk command to display information about the NVMe block devices attached to your instance.lsblk[ec2-user@mod-server1 ~]$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTxvda 202:0 0 30G 0 disk└─xvda1 202:1 0 30G 0 part /This example output shows the followingThe root volume, /dev/xvda, has a partition, /dev/xvda11. While the size of the root volume reflects the new size, 16 GB, the size of the partition reflects the original size, 8 GB, and must be extended before you can extend the file system.For volumes that have a partition, such as the root volume shown in the previous step, use the growpart command to extend the partition. Notice that there is a space between the device name and the partition number.Optional To verify that the partition reflects the increased volume size, use the lsblk command again.To extend the file system on each volume, use the correct command for your file system, as follows:XFS file system To extend the file system on each volume, use the xfs_growfs command. In this example, /sudo xfs_growfs -d /If the XFS tools are not already installed, you can install them as follows.sudo yum install xfsprogsext4 file system To extend the file system on each volume, use the resize2fs command.sudo resize2fs /dev/nvme0n1p1Other file system To extend the file system on each volume, refer to the documentation for your file system for instructions. Optional, To verify that each file system reflects the increased volume size, use the df -h command again.Further reading https://aws.amazon.com/premiumsupport/knowledge-center/expand-ebs-root-volume-windows/ https://aws.amazon.com/premiumsupport/knowledge-center/expand-root-ebs-linux/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html" }, { "title": "Vagrant and testing Ansible playbooks", "url": "/posts/Vagrant-and-testing-Ansible-playbooks/", "categories": "Ansible, howto's, linux", "tags": "ansible, linux, howto's, sysops", "date": "2022-07-03 14:00:00 +0000", "snippet": "Download VagrantYou are best going to to the main site BUTDebiancurl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"sudo apt-get update &amp;&amp; sudo apt-get install vagrantCentossudo yum install -y yum-utilssudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.reposudo yum -y install vagrantAnsible- name: Install vagrant vars: vagrant_version: 2.2.19 unarchive: src: https://releases.hashicorp.com/vagrant//vagrant__linux_amd64.zip dest: /usr/local/bin remote_src: yes mode: 0755 owner: root group: rootCreate a directory to store your Vagrant configs and move into it. Within this dir create a folder for each Vagrant box you want to create, because Vagrant will create a .vagrant per box.mkdir vagrant &amp;&amp; cd vagrant Offical Doc’sSetting update Vagrant filemkdir ubuntu-22.04-nfs &amp;&amp; cd ubuntu-22.04-nfsand create your Vagrant config filevim Vagrantfileand paste the following.Vagrant.configure(\"2\") do |config| config.vm.box = \"ubuntu/jammy64\" config.vm.hostname = \"nfs-s\" config.vm.provision \"ansible\", playbook: \"playbook.yml\" config.vm.provider :virtualbox do |v| v.name = \"nfs-s\" v.memory = 512 v.cpus = 1 v.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"] v.customize [\"modifyvm\", :id, \"--ioapic\", \"on\"] config.vm.network :private_network, ip: \"192.168.56.10\" end endThis basicly builds a Ubuntu Jammy Jellyfish (22.04) box with 512M of RAM, 1 cpu and names it nfs-s . For testing your playbooks line 4 is where it happen. Your setting Ansible as the provision and telling it what playbook to use.PlaybookLets create a playbook to set up a nfs share.vim playbook.ymland paste the below. Change the IP to what you have set up for your Vagrant box--- - hosts: all gather_facts: yes become: true tasks: - name: install nfs kernel server package: state: latest name: - nfs-kernel-server - apache2 - name: Create a nfs directory if it does not exist ansible.builtin.file: path: /srv/nfs state: directory owner: nobody group: nogroup mode: '0777' - name: Add the nfs dir to exports file ansible.builtin.lineinfile: path: /etc/exports line: /srv/nfs 192.168.56.0/24(rw,sync,no_subtree_check) create: yes - name: NFS apply change configrue shell: systemctl reload nfs;exportfs -aNow just runvagrant upYour output will look like thissam@msi:~/vagrant/vagrant-ansible/ubuntu-22.04-nfs$ vagrant upBringing machine 'default' up with 'virtualbox' provider...==&gt; default: Importing base box 'ubuntu/jammy64'...==&gt; default: Matching MAC address for NAT networking...==&gt; default: Checking if box 'ubuntu/jammy64' version '20220513.0.0' is up to date...==&gt; default: Setting the name of the VM: nfs-s==&gt; default: Clearing any previously set network interfaces...==&gt; default: Preparing network interfaces based on configuration... default: Adapter 1: nat default: Adapter 2: hostonly default: Adapter 3: hostonly default: Adapter 4: hostonly default: Adapter 5: hostonly default: Adapter 6: hostonly==&gt; default: Forwarding ports... default: 22 (guest) =&gt; 2222 (host) (adapter 1)==&gt; default: Running 'pre-boot' VM customizations...==&gt; default: Booting VM...==&gt; default: Waiting for machine to boot. This may take a few minutes... default: SSH address: 127.0.0.1:2222 default: SSH username: vagrant default: SSH auth method: private key default: default: Vagrant insecure key detected. Vagrant will automatically replace default: this with a newly generated keypair for better security. default: default: Inserting generated public key within guest... default: Removing insecure key from the guest if it's present... default: Key inserted! Disconnecting and reconnecting using new SSH key...==&gt; default: Machine booted and ready!==&gt; default: Checking for guest additions in VM... default: The guest additions on this VM do not match the installed version of default: VirtualBox! In most cases this is fine, but in rare cases it can default: prevent things such as shared folders from working properly. If you see default: shared folder errors, please make sure the guest additions within the default: virtual machine match the version of VirtualBox you have installed on default: your host and reload your VM. default: default: Guest Additions Version: 6.0.0 r127566 default: VirtualBox Version: 6.1==&gt; default: Setting hostname...==&gt; default: Configuring and enabling network interfaces...==&gt; default: Mounting shared folders... default: /vagrant =&gt; /home/sam/vagrant/vagrant-ansible/ubuntu-22.04-nfs==&gt; default: Running provisioner: ansible... default: Running ansible-playbook...PLAY [all] *********************************************************************TASK [Gathering Facts] *********************************************************ok: [default]TASK [install nfs kernel server] ***********************************************changed: [default]TASK [Create a nfs directory if it does not exist] *****************************changed: [default]TASK [Add the nfs dir to exports file] *****************************************changed: [default]TASK [NFS apply change configrue] **********************************************changed: [default]PLAY RECAP *********************************************************************default : ok=5 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 The beginning section is the Vagrant box set up. The end section is where the playbook is deployed. If there are errors with the playbook they will show up there. Nice and easy.Yes you can test your playbooks with Ansible Molecule using Vagrant, Podman and Docker, but thats for next time." }, { "title": "Spining up Uptime Kuma", "url": "/posts/spining-up-Uptime-Kuma/", "categories": "docker, howto's, monitoring", "tags": "monitoring, docker, howto's", "date": "2022-06-22 14:00:00 +0000", "snippet": "For a person who comes from a world of Zabbix and Nagios, Uptime Kuma is wicked, a light weight monitoring tool that gets out of your way to do basics, ie HTTP(S), TCP Ports, DNS &amp; SQL. Lets spin it up: You will need a Linux distro of choice and docker installedmkdir uptime_kumacd uptime_kumatouch docker-compose.ymlvim docker-compose.yml---version: \"3.1\"services: uptime-kuma: image: louislam/uptime-kuma:1 container_name: uptime-kuma volumes: - /home/serveradmin/docker_volumes/uptime-kuma/data:/app/data ports: - 3001:3001 restart: unless-stopped security_opt: - no-new-privileges:truemkdir datadocker-compose up -dDone, go to http://IP/FQDM:3001 and create your account.If you forget your passworddocker exec -it &lt;container name&gt; npm run reset-passwordDocs" }, { "title": "Create a bind server with Ubuntu", "url": "/posts/create-a-bind-server-with-ubuntu/", "categories": "homelab, howto's, linux", "tags": "homelab, linux, howto's", "date": "2022-06-19 14:00:00 +0000", "snippet": "Install BindThis guide assumes you are using Ubuntu 20.04 and have already install and setup the server.Install the require packages:sudo apt-get install bind9 bind9utils bind9-docIf you are only using IPV4 it is best practice to set bind to IPV4 mode. This is done by editing /etc/default/named file. Add “-4” to the end of the OPTIONS parameter. It should look like the following:startup options for the serverOPTIONS=\"-u bind -4\"Restart bind to apply the change with:sudo systemctl restart bind9Configuration for named.conf.optionsudo vim /etc/bind/named.conf.optionsAdd a ACL (access control list). This will be servers that can access the DNS service. In the example below the example, the ACL is called “trusted”. You can call this list anything you want.acl \"trusted\" { localhost; # This is your main host that your on 10.130.55.0/24; # This is a subnet that you will be allowing 10.130.55.12; # You can also add a host be host basis (more secure)};Under directory “/var/cache/bind”;add the following: recursion yes; # enables resursive queries allow-recursion { trusted; }; # allows recursive queries from \"trusted\" acl listen-on { hostip; }; # Private IP address of your host- listen on private network only allow-transfer { none; }; # disable zone transfers by default forwarders { 1.1.1.1; 8.8.8.8; };Not that the forwarders IP will be needed if the DNS host does not have the recorder requested and needs to look to another DNS server. You config will look something like this:acl \"trusted\" { localhost; # This is your main host that your on 10.131.55.0/24; # This is a subnet that you will be allowing 10.131.55.120; # Secondary DNS server IP (if using a second) 10.131.55.110; # You can also add a host be host basis (more secure)};options { directory \"/var/cache/bind\"; recursion yes; # enables resursive queries allow-recursion { trusted; }; # allows recursive queries from \"trusted\" acl listen-on { hostip; }; # Private IP address of your host- listen on private network only allow-transfer { none; }; # disable zone transfers by default forwarders { 1.1.1.1; 8.8.8.8; }; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. // forwarders { // 0.0.0.0; // }; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation auto; listen-on-v6 { any; };};Save and exit the file. The configuration specifies that only your servers (defined in the acl) will be able to query the DNS server for outside domains.Configuration for named.conf.localsudo vim /etc/bind/named.conf.localNote that domain.com is being used as the example domain. you need to change this to your domain.Add the forward look up zone:zone \"domain.com\" { type master; file \"/etc/bind/zones/db.domain.com\"; # zone file path allow-transfer { dns2ip; }; # Private IP address for secondary DNS server};Now add the revers lookup zonezone \"55.130.10.in-addr.arpa\" { type master; file \"/etc/bind/zones/db.10.130.55\"; # 10.130.55.0/24 subnet allow-transfer { dns2ip; }; # Private IP address for secondary DNS server};Save and exit the file.The zone configuration is now set, but we need to create the file for /etc/bind/zones/db.10.130.55 and /etc/bind/zones/db.domain.comsudo mkdir /etc/bind/zonessudo cp /etc/bind/db.local /etc/bind/zones/db.domain.comsudo vim /etc/bind/zones/db.domain.comedit the following:$TTL 604800@ IN SOA localhost. root.localhost. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL;@ IN NS localhost. ; delete this line@ IN A 127.0.0.1 ; delete this line@ IN AAAA ::1 ; delete this lineNow you need to add your NS servers:; name servers - A recordsns1.domain.com. IN A 10.130.55.10ns2.domain.com. IN A 10.130.55.11And then A records for your servers; 10.130.55.0/24 - A recordshost1.domain.com. IN A 10.130.55.100host2.domain.com. IN A 10.130.55.101Save and exit the file.Now lets create the revers lookup zone.sudo cp /etc/bind/db.127 /etc/bind/zones/db.10.130.55sudo vim /etc/bind/zones/db.10.130.55edit the following: $TTL 604800 @ IN SOA localhost. root.localhost. ( 1 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL@ IN NS localhost. ; delete this line 1.0.0 IN PTR localhost. ; delete this line Add the NS servers; name servers - NS records IN NS ns1.domain.com. IN NS ns2.domain.com.And then the PTR records; PTR Records11 IN PTR ns1.domain.com. ; 10.130.55.1012 IN PTR ns2.domain.com. ; 10.130.55.11100 IN PTR host1.domain.com. ; 10.130.55.100101 IN PTR host2.domain.com. ; 10.130.55.101Save and exit.Configuration checkCheck the zone for syntax errors with:sudo named-checkconfNow check the correctness of the zone against the domain with:sudo named-checkzone domain.com db.domain.comsudo named-checkzone 55.130.10.in-addr.arpa /etc/bind/zones/db.10.130.55If the report no errors you can now restart the bind service withsudo systemctl restart bind9Allow bind traffic on the firewall withsudo ufw allow Bind9Configuring the secondary DNS serveOn the second DNS host install the require packagessudo apt-get install bind9 bind9utils bind9-docNow edit the bind config and add:sudo vim /etc/bind/named.conf.optionsacl \"trusted\" { 10.130.55.10; # ns1 localhost; # ns2 - can be set to localhost 10.130.55.100; # single host1 10.130.55.0/24; # subnet};Then as before add: recursion yes; allow-recursion { trusted; }; listen-on { 10.130.55.11; }; # ns2 private IP address allow-transfer { none; }; # disable zone transfers by default forwarders { 1.1.1.1; 8.8.8.8; };Save and exit. Now edit options.localsudo vim /etc/bind/named.conf.localAnd add the following:zone \"domain.com\" { type slave; file \"domain.com\"; masters { 10.130.55.10; }; # ns1 private IP};zone \"55.130.10.in-addr.arpa\" { type slave; file \"db.10.130.55.\"; masters { 10.130.55.10; }; # ns1 private IP};Configuration check on secondary serverCheck the zone for syntax errors with:sudo named-checkconfIf the report no errors you can now restart the bind service withsudo systemctl restart bind9Allow bind traffic on the firewall withsudo ufw allow Bind9Now you can configure your host to check the server are working.Use tools such as nslookup and dig." }, { "title": "Converting with ffmpeg", "url": "/posts/Converting-with-ffmpeg/", "categories": "homelab, howto's, linux", "tags": "homelab, linux, howto's", "date": "2022-06-17 14:00:00 +0000", "snippet": "Convert mp4 to mkvffmpeg -i input.mp4 -codec copy output.mkv Tip: To convert all MP4 files in the current directory, run a simple loop in terminal:for i in *.mp4; do ffmpeg -i \"$i\" -codec copy \"${i%.*}.mkv\"doneCut using a specific timeffmpeg -i input.mp4 -ss 00:05:10 -to 00:15:30 -c:v copy -c:a copy output2.mp4" }, { "title": "Build a snipe-it server on Ubuntu", "url": "/posts/build-a-snipe-it-server-on-ubuntu/", "categories": "homelab, howto's, linux", "tags": "homelab, linux, howto's", "date": "2022-05-12 14:00:00 +0000", "snippet": "Install and set up LAMP Stack\\Update UbuntuUpdate your server with:sudo apt update &amp;&amp; sudo apt upgrade -dInstall Apachesudo apt install apache2 -y Enable Apache2 to run on system start up.sudo systemctl enable apache2Install MariaDBsudo apt install mariadb-serversudo systemctl start mysqlsudo systemctl enable mysqlSet up a root user password and secure MariaDB with:sudo mysql\\_secure\\_installation`Note make sure you document the password correctly.Create a database and a snipe-it user. Change to the root user with the command:sudo suAccess the MariaDB console withmysql -r root -p Create the user,database and give the user permissions to it:CREATE DATABASE snipeit_data;CREATE USER 'snipeit_user'@'localhost' IDENTIFIED BY 'PASSWORD';GRANT ALL PRIVILEGES ON snipeit.* TO 'snipeit_user'@'localhost';FLUSH PRIVILEGES;EXIT;Note to change the `PASSWORD`to the password you are giving to the user for the DB. Make sure you document this. If you want to double check your database has been created, use the following commands:mysql -u user -pMariaDB \\[(none)\\]&gt; show databases;Output:+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || snipeit |+--------------------+4 rows in set (0.000 sec)Install PHPsudo apt install php libapache2-mod-php php-mysql -yYou can check the version of the PHP installed with the following command;root@sphinx:~# php -vPHP 7.4.3 (cli) (built: Oct 6 2020 15:47:56) ( NTS )Copyright (c) The PHP GroupZend Engine v3.4.0, Copyright (c) Zend Technologies with Zend OPcache v7.4.3, Copyright (c), by Zend TechnologiesInstall Snipe-ITInstall Extra PHP Extensions and other package required.sudo apt install php-{bcmath,cli,xml,mbstring,tokenizer,curl,zip,ldap,gd} openssl curl git wget zipClone Snipe-IT from github and add it to the web dir.git clone https://github.com/snipe/snipe-it.git /var/www/html/snipeitSnipe-IT set upRename the Snipe-IT variables file .env.example to .env file.cp /var/www/html/snipeit/.env.example /var/www/html/snipeit/.envOpen the environment configuration file.vim /var/www/html/snipeit/.envSnipe-IT Application SettingsSet the URL you will use to access your Snipe-IT, the Application language, Timezone. The URL should not have a trailing slash.\\# --------------------------------------------\\# REQUIRED: BASIC APP SETTINGS\\# --------------------------------------------APP_ENV=productionAPP_DEBUG=falseAPP_KEY=ChangeMeAPP_URL=[http://servername.domain.com](http://servername.domain.com/)APP_TIMEZONE='Europe/London'APP_LOCALE=enSnipe-IT Database SettingsSet the database host, database name, user and password created above,\\# --------------------------------------------\\# REQUIRED: DATABASE SETTINGS\\# --------------------------------------------DB_CONNECTION=mysqlDB_HOST=127.0.0.1DB_DATABASE=snipeitDB\\_USERNAME=snipeit\\_userDB_PASSWORD=P@SSWORDDB_PREFIX=nullDB\\_DUMP\\_PATH='/usr/bin'DB_CHARSET=utf8mb4DB\\_COLLATION=utf8mb4\\_unicode_ciIf you are gonna need to send emails, configure Email Server setting as well.Install Required PHP Librariescd /var/www/html/snipeitcurl -sS https://getcomposer.org/installer | phpphp composer.phar install --no-dev --prefer-sourceSet dir ownership and given permissionchown -R www-data:www-data /var/www/html/snipeitGenerate Snipe-IT App KeyThe app key is a randomly generated key that app uses to encrypt data. The value generated will be assigned automatically to APP_KEY variable in the Snipe-IT configuration file.php artisan key:generate\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* Application In Production! *\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*Do you really wish to run this command? (yes/no) \\[no\\]:\\&gt; yesApplication key set successfully.The key generated is automatically set as the value of the APP_KEY variable in the .env file.Configure Apache with SSLNote you will need to enable ssl for apache with:sudo a2enmod sslvim /etc/apache2/sites-available/snipeit.conf&amp;lt;VirtualHost *:443&amp;gt; DocumentRoot /var/www/html/snipeit/public ServerName SERVERNAME.domain.com SSLEngine on SSLCertificateFile /etc/apache2/ssl/NAMEOFCERT.cer SSLCertificateKeyFile /etc/apache2/ssl/NAMEOFCERT.key SSLCertificateChainFile /etc/apache2/ssl/NAMEOFCERT.cer &amp;lt;Directory /var/www/html/snipeit/public&amp;gt; Allow From All AllowOverride None Options None &amp;lt;/Directory&amp;gt; RewriteEngine On RewriteCond %{DOCUMENT\\_ROOT}%{REQUEST\\_FILENAME} !-d RewriteCond %{REQUEST_URI} (.+)/$ RewriteRule ^ %1 \\[L,R=301\\] RewriteCond %{DOCUMENT\\_ROOT}%{REQUEST\\_FILENAME} !-d RewriteCond %{DOCUMENT\\_ROOT}%{REQUEST\\_FILENAME} !-f RewriteRule ^ /index.php \\[L\\] ErrorLog ${APACHE\\_LOG\\_DIR}/snipeit-error.log CustomLog ${APACHE\\_LOG\\_DIR}/snipeit-access.log combined&amp;lt;/VirtualHost&amp;gt;If you are not using SSL, then the config will be like this:&amp;lt;VirtualHost *:80&amp;gt; \tDocumentRoot /var/www/html/snipeit/public\tServerName SERVERNAME.domain.com\t&amp;lt;Directory /var/www/html/snipeit/public&amp;gt;\t\tAllow From All\t\tAllowOverride None\t\tOptions None\t&amp;lt;/Directory&amp;gt;\tRewriteEngine On\tRewriteCond %{DOCUMENT\\_ROOT}%{REQUEST\\_FILENAME} !-d\tRewriteCond %{REQUEST_URI} (.+)/$\tRewriteRule ^ %1 \\[L,R=301\\]\tRewriteCond %{DOCUMENT\\_ROOT}%{REQUEST\\_FILENAME} !-d\tRewriteCond %{DOCUMENT\\_ROOT}%{REQUEST\\_FILENAME} !-f\tRewriteRule ^ /index.php \\[L\\]\tErrorLog ${APACHE\\_LOG\\_DIR}/snipeit-error.log\tCustomLog ${APACHE\\_LOG\\_DIR}/snipeit-access.log combined&amp;lt;/VirtualHost&amp;gt;Save the configuration file and run a file syntax test.apachectl configtestIf the command return, Syntax Ok, proceed. Otherwise fix the would errors.Enable Snipe-IT site configuration.a2ensite snipeit.confEnable Rewrite module.a2enmod rewriteRestart Apachesystemctl restart apache2Allow through the firewallsudo ufw allow 443sudo ufw reloadSnipe-IT Pre-Flight &amp; Setup Run the pre-flight setup to verify that all configurations are correct. You can access Snipe-IT pre-flight page from the URL; http://&lt;Hostname or Address&gt;." }, { "title": "Using Fail2Ban", "url": "/posts/using-fail2ban/", "categories": "homelab, howto's, linux, security", "tags": "homelab, linux, howto's, security", "date": "2022-04-26 14:00:00 +0000", "snippet": "Install and enable fail2bansudo systemctl start fail2bansudo systemctl enable fail2banEdit the jailssudo vim /etc/fail2ban/jail.localRestart fail2bansudo systemctl restart fail2banCheck status for all jailssudo fail2ban-client statusCheck status for a certain jailsudo fail2ban-client status sshdUn-ban a IPsudo fail2ban-client set sshd unbanip XXX.XXX.XXX.XXX" }, { "title": "Build a iboard server", "url": "/posts/build-a-iboaed-server/", "categories": "homelab, howto's, linux, security", "tags": "homelab, linux, howto's, security", "date": "2022-04-16 14:00:00 +0000", "snippet": "Install apache2.sudo apt install apache2Install make.sudo apt install makeDownload the iboard repo.git clone https://github.com/Harrison-S1/iboard.gitMove into the iboard dir and install the required perl mods.cd CGI-4.51/; perl Makefile.PL; make; make installcd Date-EzDate-1.16/; perl Makefile.PL; make; make installEnable CGI on apache.cd /etc/apache2/mods-enabledsudo ln -s ../mods-available/cgi.loadorsudo a2enmod cgiCopy the following files to /usr/lib/cgi-bin/ryansiob .-rwxr-xr-x 3 root www-data 914 Oct 29 15:38 envelope.gif*-rwxr-xr-x 1 root root 30900 Oct 29 16:26 ryansiob.config.pl*-rwxr-xr-x 1 root root 8223 Oct 29 15:38 ryansiob.pl*-rwxr-xr-x 1 root root 2274 Oct 29 15:38 ryansiob.search.pl*Change the group of envelope.gif.chown root:www-data envelope.gifMake .pl exacutable.chmod +x /usr/lib/cgi-bin/ryansiob/*Copy the following files to /usr/local/ryansiob.-rwxrwx--- 1 root www-data 2229 Oct 29 09:26 allout.pl*-rwxrwx--- 1 root www-data 1100 Oct 29 16:14 datafile*-rwxrwx--- 1 www-data www-data 1100 Oct 29 17:40 datafile.bak*Change the group of the perant folder - apache needs to be able to read and write here so change there permissions as well.chown -R root:www-data /usr/local/ryansiob/chmod -R 770 /usr/local/ryansiob/Create a link for the envelope.gif in the root of the /var/www/html.ln /usr/lib/cgi-bin/ryansiob/envelope.gif /var/www/html/Check permission have followed.-rwxr-xr-x 3 root www-data 914 Oct 29 15:38 envelope.gif*-rw-r--r-- 1 root root 10918 Oct 29 17:28 index.html.ogThe site refreshes every 30 seconds, but does this based on the URL. Change the url with:sed -i \"s|localhost/|$HOSTNAME.domain.com|g\"Allow port 80 on the firewall.sudo ufw allow 80Restart apache.sudo systemctl restart apache2In the web browser go to:http://FQDN/cgi-bin/ryansiob/ryansiob.plSoftware source. Ryansiob CGI-4.51 Date-EzDate-1.16" }, { "title": "RDP to Debian", "url": "/posts/rdp-to-debian/", "categories": "homelab, howto's, linux, security", "tags": "homelab, linux, howto's, security", "date": "2022-03-15 14:00:00 +0000", "snippet": "Install xrdp.sudo apt install xrdpNow make sure to automatically start the service at the system startup.systemctl enable xrdpThen make sure that the service is running.systemctl status xrdpIf you are running a firewall make sure you have put a allow rule in for the RDP port.ufw allow 3389/tcpNoteYou have to configure the polkit rule to avoid an authenticate popup after inputting the username and password at the xrdp login screen on windows.vim /etc/polkit-1/localauthority.conf.d/02-allow-colord.conf polkit.addRule(function(action, subject) {if ((action.id == “org.freedesktop.color-manager.create-device” || action.id == “org.freedesktop.color-manager.create-profile” || action.id == “org.freedesktop.color-manager.delete-device” || action.id == “org.freedesktop.color-manager.delete-profile” || action.id == “org.freedesktop.color-manager.modify-device” || action.id == “org.freedesktop.color-manager.modify-profile”) &amp;&amp; subject.isInGroup(“{group}”)){return polkit.Result.YES;}});Restart the xrdp service.systemctl restart xrdp" }, { "title": "Setting up Nessus Pro", "url": "/posts/setting-up-nessus-pro/", "categories": "homelab, howto's, linux, security", "tags": "homelab, linux, howto's, security", "date": "2022-03-10 14:00:00 +0000", "snippet": " Build your server Download the package for your server. https://www.tenable.com/downloads/nessus For this example a Ubuntu server is being used. Copy the package to your service and for Ubuntu run the following command.sudo dpkg -i Nessus*.deband run the following command to start the daemon/bin/systemctl start nessusd.service Navigate to https://FQDN:8834/ Run through the set up to complete the build. Nessus files are stored in /opt/ Useful commands for Ubuntu/bin/systemctl start nessusd.serviceStarts the daemon/bin/systemctl stop nessusd.serviceStops the daemon/bin/systemctl restart nessusd.serviceRestarts the daemonSSL certs are stored:/opt/nessus/com/nessus/CA/servercert.pemand/opt/nessus/var/nessus/CA/serverkey.pemUse the following commands to start or stop the service:Note that you would use these command with these system. The latest Linux distros are using systemctl not init.RedHat, CentOS, and Oracle LinuxStart/sbin/service nessusd startStop/sbin/service nessusd stopSUSEStart/etc/rc.d/nessusd startStopetc/rc.d/nessusd stopFreeBSDStartvice nessusd startStopvice nessusd stopDebian, Kali, and UbuntuStartetc/init.d/nessusd startStopetc/init.d/nessusd stopUpdate or add ssl certBack up the original Nessus CA and server certificates and keys:cp /opt/nessus/com/nessus/CA/cacert.pem /opt/nessus/com/nessus/CA/cacert.pem.origcp /opt/nessus/var/nessus/CA/cakey.pem /opt/nessus/var/nessus/CA/cakey.pem.origcp /opt/nessus/com/nessus/CA/servercert.pem /opt/nessus/com/nessus/CA/servercert.pem.origcp /opt/nessus/var/nessus/CA/serverkey.pem /opt/nessus/var/nessus/CA/serverkey.pem.origReplace the original certificates with the new custom certificates:cp customCA.pem /opt/nessus/com/nessus/CA/cacert.pemcp customCA.key /opt/nessus/var/nessus/CA/cakey.pemcp servercert.pem /opt/nessus/com/nessus/CA/servercert.pemcp server.key /opt/nessus/var/nessus/CA/serverkey.pemRestart Nessus:service nessusd restartIf you get stuck: https://docs.tenable.com/nessus/Content/UploadACustomCACertificate.htm" }, { "title": "How to Enable Process Accounting in Ubuntu", "url": "/posts/enable-process-accounting-ubuntu/", "categories": "ubuntu, howto's, linux, security", "tags": "security, ubuntu, howto's", "date": "2022-02-09 14:00:00 +0000", "snippet": "Acct will log user processIf you Enable process accounting in your system, it will help you to keep track of your user processes. It is very useful for System administrators for keeping log of your users. In Ubuntu Process accounting can be done by installing utility called Acctsudo apt-get install acct Make a log file for process accountingsudo touch /var/log/pacctEnable process accounting onsudo accton /var/log/pacctor/etc/init.d/acct startFor viewing the Process Information Use the following command:Display details about users’ connect timeac ac command displays a report of connect time in hours based on the logins/logouts. ac - Print total connection time. ac -dp - display daily (-d) connection totals by person (-p)Display information about previously executed user commandsThe below command will display the commands executed by user johnsudo lastcomm johnSearch and display log by command rmsudo lastcomm rmSearch and display log by terminal namesudo lastcomm pts/1 Print Accounting statistics sudo sasa command will display information about previously executed commands, The information can also be summarized on a per-user basis The output fields are labelled as follows: cpu sum of system and user time in cpu seconds re “real time” in cpu seconds k cpu-time averaged core usage, in 1k units avio average number of I/O operations per execution t- io total number of I/O operations k*sec cpu storage integral (kilo-core seconds) u user cpu time in cpu seconds s system time in cpu secondsDisplay ouput per usersudo sa -u Display the number of processes and number of CPU minutes on a per-user basissudo sa -mBy using sa command and looking at re, k, cp/cpu time you can find out suspicious activity or user and command who is eating your CPU and Memory . An increase in CPU/memory usage is indication of problem." }, { "title": "Backup & restore MYSQL MariaDB databases", "url": "/posts/Backup-&-restore-MYSQL-MariaDB-databases/", "categories": "homelab, howto's, linux", "tags": "homelab, ubuntu, howto's, backups", "date": "2022-02-09 14:00:00 +0000", "snippet": "Backing upmysqldump --add-drop-table --databases mydatabase_db &gt; /home/wiki/db/$(/bin/date +\\\\%Y-\\\\%m-\\\\%d).sql.bak mysqldump is the backup tool Read More add-drop-table removes one or more tables Read More –databases is defining the db name, i.e.mydatabase_db &gt; Forward this output to the /path/the/folderRestoringmysql -u root mydatabase_db &lt; 2019-09-10.sql.bakYour location my be more like /path/to/file/2019-09-10.sql.bak" }, { "title": "Resize Ubuntu disk on VMware", "url": "/posts/resize-ubuntu-disk-on-vmware/", "categories": "cloud, howto's, linux", "tags": "cloud, vmware, ubuntu, howto's", "date": "2022-02-07 14:00:00 +0000", "snippet": " Add more storage to the drive in Vmware and reboot the server. SSH into the server once back up, OR use the console. /sda being the disk name/ ;abel given by the system. Note the space between sda &amp; 1 is not a type-oGrow the partitionsudo growpart /dev/sda 1Rezise the partitionsudo resize2fs /dev/sda1Check the disk sizedf -h to checkThis also works with Google Cloud instances." }, { "title": "Ansible creating a inventory list", "url": "/posts/Ansible-creating-a-inventory-list/", "categories": "Ansible, howto's, linux", "tags": "ansible, linux, howto's, sysops", "date": "2021-06-10 14:00:00 +0000", "snippet": "Create a inventory.yml file in your users ansible filevim inventory.ymlAnd use the layout below. All yaml files start with — all: – All server will be in this group. children: –Is saying that there a another group within all: linux: – This is the group name that you want to call your group. host: – Is the hosts that belong to this group.In the example below: All server are in the all: group. The all: group has a group inside it called linux. The group linux also have it’s own groups call ubuntu and centos. www is another group that is sits in line with the linux group---all: children: linux: children: debian: hosts: example1.domain.com: example2.domain.com: rhel: hosts: example3.vog.com www: hosts: example2.domain.com: example3.domain.com:To view the host user the following commandansible linux --list-hosts -i inventory.ymlFurther reading" }, { "title": "Ansible adding a new servers", "url": "/posts/Ansible-sdding-a-new-servers/", "categories": "Ansible, howto's, linux", "tags": "ansible, linux, howto's, sysops", "date": "2021-06-01 14:00:00 +0000", "snippet": "Copying SSH-keyOn your new server, create the management user. Password for this user is in Keepass.Once done, log into the management server as the management user. You need to copy over the ssh key for this user to the new server.cd .sshssh-copy-id -i id_ed25519.pub USER@SERVERTest the key is working by logging onto the new server from the management server.Append the ssh confBack on the management server in the .ssh folder edit the config filevim configAnd add in the following syntaxHost WHAT-YOU-CALLED-THE-HOST Hostname HOSTNAME-OF-SERVER Port SSH-PORT-NUMBER-BEING-USED User MANAGMENT-USERNAMEBy added the server into this file you will not be prompted for a username or need to give the port number that is being used for sshAdding to the inventorycd ansible/Edit the inventory filevim inventory.ymlAdd the host according to the syntax showing in this file. Once saved you can now test that the host is responding with:ansible webservers -m pingNote that “webservers” needs to be replaced with the parent of where you have placed the new server in the inventory.yml file" }, { "title": "Ansible basics", "url": "/posts/ansbile-basics/", "categories": "Ansible, howto's, linux", "tags": "ansible, linux, howto's, sysops", "date": "2021-05-25 14:00:00 +0000", "snippet": "Example CMDCommand can be run as a user with sudo rights.ansible-playbook updateubuntu.yml --ask-sudo-passBy adding the -k the users ssh password will be promoted for. Use if SSH keys are not set up.ansible-playbook -k updateubuntu.yml --ask-sudo-passAd-hoc commandsNoteAction-aModule-mEscalate privilege-bCommand Doesnt use the shell (Bash/sh) Can not use pipes or redirectsansible all -b -m command -a 'echo \"hello\" &gt; /root/hello.txt'Shell Supports pipes and redirects Can get messed up by user settingsansible all -b -m shell -a 'echo \"hello\" &gt; /root/hello.txt'Raw Just sends a command over ssh Doesn’t need pythonansible all -b -m raw -a 'echo \"hello\" &gt; /root/hello.txt'To remove a fileansible all -b -m file -a 'path=/root/hello.txt state=abset'SyntaxTo run an ad hoc command, the command must be framed or have the following syntax.ansible &amp;lt;host-pattern&amp;gt; \\[options\\]for example. the command should be written as follows.ansible appserverhostgroup -m &amp;lt;modulename&amp;gt; -a &amp;lt;arguments to the module&amp;gt;A single ansible ad hoc command can have multiple options. -m and -a are one amongst them and widely used.List hostsansible all --list-hostsPing hostsif you do not have SSH key-based authenticationansible multi -m ping -i ansible_hosts --user=vagrant --ask-passShow host uptime for allansible -m raw -a '/usr/bin/uptime' alloransible all -a 'uptime'Restart servicesansible all -b -m service -a 'name=apache2 state=started'Check python verison on all hostsansible -m shell -a 'python -V' allCheck free memoryansible multi -a \"free -m\" -i inventory.ymlCreate a useransible app -m user -a \"name=weblogic group=weblogic createhome=yes\" -bCreate a user groupansible app -s -m group -a \"name=weblogic state=present\"Create a Directory with 755 permissionansible app -m file -a \"path=/opt/oracle state=directory mode=0755\" -bCreate a file with 755 permissionansible app -m file -a \"path=/tmp/testfile state=touch mode=0755\"Check free disk spaceansible multi -a \"df -h\"Start or stop the serviceTo Startansible multi -s -m service -a \"name=httod state=started enabled=yes\"To Stopansible multi -s -m service -a \"name=httpd state=stop enabled=yes\"Install a package using apt commandansible -i inventory.yml ubuntu -m raw -a \"apt install -y ntp\"oransible -i inventory.yml ubuntu -a 'apt install ntp'oransible -i inventory.yml ubuntu -m apt -a 'name=ntp state=present'Install a package using yum commandansible - ineventory.yml centos -m yum -a \"name=httpd state=present\"Note: Status can = lastest which will update the package to the latest version if it is present, but will also install the package if it is not. To remove the package, the state would = absentManaging Cron Job and SchedulingRun the job every 15 minutesansible multi -s -m cron -a \"name='daily-cron-all-servers' minute=*/15 job='/path/to/minute-script.sh'\"Run the job every four hoursansible multi -s -m cron -a \"name='daily-cron-all-servers' hour=4 job='/path/to/hour-script.sh'\"Enabling a Job to run at system rebootansible multi -s -m cron -a \"name='daily-cron-all-servers' special_time=reboot job='/path/to/startup-script.sh'\"Scheduling a Daily jobansible multi -s -m cron -a \"name='daily-cron-all-servers' special_time=daily job='/path/to/daily-script.sh'\"Scheduling a Weekly jobansible multi -s -m cron -a \"name='daily-cron-all-servers' special_time=weekly job='/path/to/daily-script.shCopy filesansible all -b -m copy -a 'src=/etc/hosts dest=/etc/hosts'Note the source will copy the file on the source client to the servers.Fetch filesansible HOST -m fetch -a 'src=/path/to/soruce.txt dest=/home/USER/FOLDER/ flat=yes'" }, { "title": "Rkhunter - A Linux Rootkit Scanner", "url": "/posts/rkhunter-linux-rootkit-scanner/", "categories": "security, howto's, linux", "tags": "security, linux, howto's", "date": "2021-04-24 14:00:00 +0000", "snippet": "RKH (RootKit Hunter) is a free, open source, powerful, simple to use and well known tool for scanning backdoors, rootkits and local exploits on POSIX compliant systems such as Linux. As the name implies, it is a rootkit hunter, security monitoring and analyzing tool that is thoroughly inspects a system to detect hidden security holes.The rkhunter tool can be installed using following command on Ubuntu and CentOS based systems.Debian basesudo apt install rkhunter Rhel baseyum install epel-release yum install rkhunterTo check your server with rkhunter run the following command.rkhunter -cTo make run rkhunter automatically at every night, add the following cron entry, which will run at 3am night and send reports to your email address.0 3 * * * /usr/sbin/rkhunter -c 2&gt;&amp;1 | mail -s \"rkhunter Reports of My Server\" you@yourdomain.comFor more information and options run the following command.rkhunter --helphttps://en.wikipedia.org/wiki/Rkhunterhttps://sourceforge.net/projects/rkhunter/https://wiki.archlinux.org/title/Rkhunter" } ]
